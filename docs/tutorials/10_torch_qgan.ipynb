{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch qGAN Implementation\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm.\n",
    "\n",
    "## Context\n",
    "\n",
    "The qGAN \\[1\\] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator $G_{\\theta}$, i.e., an ansatz, and a classical discriminator $D_{\\phi}$, a neural network, to learn the underlying probability distribution given training data.\n",
    "\n",
    "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating samples that will be classified by the discriminator as training data samples (i.e, samples extracted from the real training distribution), and the discriminator tries to differentiate between original training data samples and data samples from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the training data's underlying probability distribution.\n",
    "The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution.\n",
    "\n",
    "**References:**\n",
    "\n",
    "\\[1\\] Zoufal et al., [Quantum Generative Adversarial Networks for learning and loading random distributions](https://www.nature.com/articles/s41534-019-0223-2)\n",
    "\n",
    "### Application: qGANs for Loading Random Distributions\n",
    "\n",
    "Given $k$-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data's underlying random distribution and to load it directly into a quantum state:\n",
    "\n",
    "$$ \\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle $$\n",
    "\n",
    "where $p_{\\theta}^{j}$ describe the occurrence probabilities of the basis states $\\big| j\\rangle$.\n",
    "\n",
    "The aim of the qGAN training is to generate a state $\\big| g_{\\theta}\\rangle$ where $p_{\\theta}^{j}$, for $j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}$, describe a probability distribution that is close to the distribution underlying the training data $X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}$.\n",
    "\n",
    "For further details please refer to [Quantum Generative Adversarial Networks for Learning and Loading Random Distributions](https://arxiv.org/abs/1904.00043) _Zoufal, Lucchi, Woerner_ \\[2019\\].\n",
    "\n",
    "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the\n",
    "[Option Pricing with qGANs](https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb) tutorial.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "### Data and Representation\n",
    "\n",
    "First, we need to load our training data $X$.\n",
    "\n",
    "In this tutorial, the training data is given by samples from a 2D multivariate normal distribution.\n",
    "\n",
    "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an $n$-qubit quantum state\n",
    "\\begin{equation}\n",
    "|g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle,\n",
    "\\end{equation}\n",
    "where the basis states $|x_{j}\\rangle$ represent the data items in the training data set\n",
    "$X={x_0, \\ldots, x_{k-1}}$ with $k\\leq 2^n$ and $p_j$ refers to the sampling probability\n",
    "of $|x_{j}\\rangle$.\n",
    "\n",
    "To facilitate this representation, we need to map the samples from the multivariate\n",
    "normal distribution to discrete values. The number of values that can be represented\n",
    "depends on the number of qubits used for the mapping.\n",
    "Hence, the data resolution is defined by the number of qubits.\n",
    "If we use $3$ qubits to represent one feature, we have $2^3 = 8$ discrete values.\n",
    "\n",
    "We first begin by fixing seeds in the random number generators, then we will import the libraries and packages we will need for this tutorial."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from qiskit.utils import algorithm_globals\n",
    "\n",
    "torch.manual_seed(42)\n",
    "algorithm_globals.random_seed = 42"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we sample some data from a 2D multivariate normal distribution as described above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from qiskit_machine_learning.datasets.dataset_helper import discretize_and_truncate\n",
    "\n",
    "# Load the training data\n",
    "training_data = algorithm_globals.random.multivariate_normal(\n",
    "    mean=[0.0, 0.0], cov=[[1, 0], [0, 1]], size=1000, check_valid=\"warn\", tol=1e-8, method=\"svd\"\n",
    ")\n",
    "# Define minimal and maximal values for the training data\n",
    "bounds_min = np.percentile(training_data, 5, axis=0)\n",
    "bounds_max = np.percentile(training_data, 95, axis=0)\n",
    "bounds = []\n",
    "for i, _ in enumerate(bounds_min):\n",
    "    bounds.append([bounds_min[i], bounds_max[i]])\n",
    "\n",
    "# Determine data resolution for each dimension of the training data in terms\n",
    "# of the number of qubits used to represent each data dimension.\n",
    "data_dim = [3, 3]\n",
    "\n",
    "# Pre-processing, i.e., discretization of the data (gridding)\n",
    "(training_data, _, grid_elements, prob_data) = discretize_and_truncate(\n",
    "    training_data,\n",
    "    np.asarray(bounds),\n",
    "    data_dim,\n",
    "    return_data_grid_elements=True,\n",
    "    return_prob=True,\n",
    "    prob_non_zero=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We convert data arrays into tensors and create a data loader from our training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data = torch.tensor(training_data, dtype=torch.float)\n",
    "grid_elements = torch.tensor(grid_elements, dtype=torch.float)\n",
    "\n",
    "# Define the training batch size\n",
    "batch_size = 300\n",
    "dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backend Configurations\n",
    "Next, we need to choose a backend that is used to run the quantum generator.\n",
    "The presented method is compatible with all shot-based backends (qasm, fake hardware, real hardware) provided by Qiskit.\n",
    "\n",
    "First, we create a quantum instance for the training where the batch size defines the number of shots."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit import Aer\n",
    "from qiskit.utils import QuantumInstance\n",
    "\n",
    "backend = Aer.get_backend(\"aer_simulator\")\n",
    "qi_training = QuantumInstance(backend, shots=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we create a quantum instance for the evaluation purposes, we choose a higher number of shots to get better insights."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qi_sampling = QuantumInstance(backend, shots=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the quantum neural network ansatz\n",
    "\n",
    "Now, we define the parameterized quantum circuit $G\\left(\\boldsymbol{\\theta}\\right)$ with $\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}$ which will be used in our quantum generator.\n",
    "\n",
    "To implement the quantum generator, we choose a depth-$2$ ansatz that implements $R_Y$ rotations and $CX$ gates which takes a uniform distribution as an input state. Notably, for $k>1$ the generator's parameters must be chosen carefully. For example, the circuit depth should be more than $1$ because higher circuit depths enable the representation of more complex structures.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "\n",
    "# sum(data_dim) corresponds to the total number of qubits in our quantum circuit (qc)\n",
    "qc = QuantumCircuit(sum(data_dim))\n",
    "qc.h(qc.qubits)\n",
    "\n",
    "# We choose a hardware efficient ansatz.\n",
    "twolocal = TwoLocal(sum(data_dim), \"ry\", \"cx\", reps=2, entanglement=\"sca\")\n",
    "qc.compose(twolocal, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's draw our circuit and see what it looks like."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qc.decompose().draw(\"mpl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the quantum generator\n",
    "\n",
    "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. As parameters this function takes a quantum instance to be used for data sampling. We wrap a created quantum neural network in `TorchConnector` to make use of PyTorch-based training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "\n",
    "\n",
    "def create_generator(quantum_instance) -> TorchConnector:\n",
    "    circuit_qnn = CircuitQNN(\n",
    "        qc,\n",
    "        input_params=[],\n",
    "        weight_params=qc.parameters,\n",
    "        quantum_instance=quantum_instance,\n",
    "        sampling=True,\n",
    "        sparse=False,\n",
    "        interpret=lambda x: grid_elements[x],\n",
    "    )\n",
    "\n",
    "    return TorchConnector(circuit_qnn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the classical discriminator\n",
    "\n",
    "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear_input = nn.Linear(input_size, 20)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.linear20 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_input(input)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.linear20(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the loss functions\n",
    "We want to train the generator and the discriminator with binary cross entropy as loss function:\n",
    "$$L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],$$\n",
    "where $x_j$ refers to a data sample and $y_j$ to the corresponding label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generator loss function\n",
    "gen_loss_fun = nn.BCELoss()\n",
    "\n",
    "# Discriminator loss function\n",
    "disc_loss_fun = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of custom gradients for the generator BCE loss function\n",
    "The evaluation of custom gradients for the quantum generator requires us to combine quantum gradients $\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}$ that we compute with Qiskit's gradient framework with the binary cross entropy as follows:\n",
    "$$\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].$$\n",
    "\n",
    "First, we need to define how gradients will be evaluated. Depending on the backend, the gradients may be returned in a sparse format. Since PyTorch provides only a limited support for sparse gradients, we need to write a custom function for gradient based training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit.opflow import Gradient, StateFn\n",
    "\n",
    "generator_grad = Gradient().gradient_wrapper(\n",
    "    StateFn(qc), twolocal.ordered_parameters, backend=qi_training\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define the custom function that evaluates gradients of the generator loss function considering the custom gradients of the quantum generator. As the parameters the function takes a list of parameter values and the discriminator as a classical neural network. The function returns a list of gradient values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generator_loss_grad(parameter_values, discriminator):\n",
    "    # evaluate gradient\n",
    "    grads = generator_grad(parameter_values).tolist()\n",
    "\n",
    "    loss_grad = ()\n",
    "    for j, grad in enumerate(grads):\n",
    "        cx = grad[0].tocoo()\n",
    "        input = torch.zeros(len(cx.col), len(data_dim))\n",
    "        target = torch.ones(len(cx.col), 1)\n",
    "        weight = torch.zeros(len(cx.col), 1)\n",
    "\n",
    "        for i, (index, prob_grad) in enumerate(zip(cx.col, cx.data)):\n",
    "            input[i, :] = grid_elements[index]\n",
    "            weight[i, :] = prob_grad\n",
    "        bce_loss_grad = F.binary_cross_entropy(discriminator(input), target, weight)\n",
    "        loss_grad += (bce_loss_grad,)\n",
    "    loss_grad = torch.stack(loss_grad)\n",
    "    return loss_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relative entropy as benchmarking metric\n",
    "The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution.\n",
    "\n",
    "In the next function we computes relative entropy between target and trained distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_relative_entropy(gen_data) -> float:\n",
    "    prob_gen = np.zeros(len(grid_elements))\n",
    "    for j, item in enumerate(grid_elements):\n",
    "        for gen_item in gen_data.detach().numpy():\n",
    "            if np.allclose(np.round(gen_item, 6), np.round(item, 6), rtol=1e-5):\n",
    "                prob_gen[j] += 1\n",
    "    prob_gen = prob_gen / len(gen_data)\n",
    "    prob_gen = [1e-8 if x == 0 else x for x in prob_gen]\n",
    "    return entropy(prob_gen, prob_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the optimizers\n",
    "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see [Kingma et al., Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980) for more details."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = create_generator(qi_training)\n",
    "discriminator = Discriminator(len(data_dim))\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "b1 = 0.9  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "num_epochs = 10  # number of training epochs\n",
    "\n",
    "# optimizer for the generator\n",
    "optimizer_gen = Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "# optimizer for the discriminator\n",
    "optimizer_disc = Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization of the training process\n",
    "We will visualize what is happening during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete.\n",
    "\n",
    "Visualization of the training process begins when training data is collected across two epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_training_progress():\n",
    "    # we don't plot if we don't have enough data\n",
    "    if len(generator_loss_values) < 2:\n",
    "        return\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Loss\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.plot(generator_loss_values, label=\"generator loss\", color=\"royalblue\")\n",
    "    ax1.plot(discriminator_loss_values, label=\"discriminator loss\", color=\"magenta\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid()\n",
    "\n",
    "    # Relative Entropy\n",
    "    ax2.set_title(\"Relative entropy\")\n",
    "    ax2.plot(relative_entropy_values)\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"Relative entropy\")\n",
    "    ax2.grid()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Now, we are ready to train our model. It may take some time to train the model so be patient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Relative entropy list\n",
    "relative_entropy_values = []\n",
    "# Generator loss list\n",
    "generator_loss_values = []\n",
    "# Discriminator loss list\n",
    "discriminator_loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    relative_entropy_epoch = []\n",
    "    generator_loss_epoch = []\n",
    "    discriminator_loss_epoch = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(data.size(0), 1)\n",
    "        fake = torch.zeros(data.size(0), 1)\n",
    "\n",
    "        # Generate a batch of data points\n",
    "        gen_data = generator()\n",
    "\n",
    "        # Evaluate Relative Entropy\n",
    "        relative_entropy_epoch.append(get_relative_entropy(gen_data))\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        # Loss measures discriminator's ability to distinguish real from generated samples\n",
    "        disc_data = discriminator(data)\n",
    "\n",
    "        real_loss = disc_loss_fun(disc_data, valid)\n",
    "        fake_loss = disc_loss_fun(discriminator(gen_data), fake)\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_gen.zero_grad()\n",
    "\n",
    "        # Loss measures generator's ability to prepare good data samples\n",
    "        generator_loss = gen_loss_fun(discriminator(gen_data), valid)\n",
    "        generator_loss.retain_grad = True\n",
    "        g_loss_grad = generator_loss_grad(generator.weight.data.numpy(), discriminator)\n",
    "\n",
    "        # generator_loss.backward(retain_graph=True)\n",
    "        for j, param in enumerate(generator.parameters()):\n",
    "            param.grad = g_loss_grad\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        generator_loss_epoch.append(generator_loss.item())\n",
    "        discriminator_loss_epoch.append(discriminator_loss.item())\n",
    "\n",
    "    relative_entropy_values.append(np.mean(relative_entropy_epoch))\n",
    "    generator_loss_values.append(np.mean(generator_loss_epoch))\n",
    "    discriminator_loss_values.append(np.mean(discriminator_loss_epoch))\n",
    "\n",
    "    plot_training_progress()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results: cumulative distribution functions\n",
    "In the final section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution.\n",
    "\n",
    "We create a new generator for sampling that takes more shots and, hence, gives more information. Recall, the sampling quantum instance was created with a larger number of shots. Then, we set the weights of the sampling generator to the values obtained in the training process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator_sampling = create_generator(qi_sampling)\n",
    "generator_sampling.weight.data = generator.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we wet the generator data samples, corresponding sampling probabilities, and plot the cumulative distribution functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gen_data = generator_sampling().detach().numpy()\n",
    "prob_gen = np.zeros(len(grid_elements))\n",
    "for j, item in enumerate(grid_elements):\n",
    "    for gen_item in gen_data:\n",
    "        if np.allclose(np.round(gen_item, 6), np.round(item, 6), rtol=1e-5):\n",
    "            prob_gen[j] += 1\n",
    "prob_gen = prob_gen / len(gen_data)\n",
    "\n",
    "prob_gen = [1e-8 if x == 0 else x for x in prob_gen]\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "ax1.set_title(\"Cumulative Distribution Function\")\n",
    "# there's a known issue in matplotlit with placing a legend on the 3d plot\n",
    "ax1.bar3d(\n",
    "    np.transpose(grid_elements)[1],\n",
    "    np.transpose(grid_elements)[0],\n",
    "    np.zeros(len(prob_gen)),\n",
    "    0.05,\n",
    "    0.05,\n",
    "    np.cumsum(prob_gen),\n",
    "    label=\"generated data\",\n",
    "    color=\"blue\",\n",
    "    alpha=1,\n",
    ")\n",
    "ax1.bar3d(\n",
    "    np.transpose(grid_elements)[1] + 0.05,\n",
    "    np.transpose(grid_elements)[0] + 0.05,\n",
    "    np.zeros(len(prob_data)),\n",
    "    0.05,\n",
    "    0.05,\n",
    "    np.cumsum(prob_data),\n",
    "    label=\"training data\",\n",
    "    color=\"orange\",\n",
    "    alpha=1,\n",
    ")\n",
    "# ax1.legend(loc='upper right')\n",
    "ax1.set_xlabel(\"x_1\")\n",
    "ax1.set_ylabel(\"x_0\")\n",
    "ax1.set_zlabel(\"p(x)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the plot above, in the <span style=\"color:blue\">blue color</span> the generated distribution is shown and\n",
    "in the <span style=\"color:orange\">orange color</span> the training one. You may find that both CDFs are similar to each other.|"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "Quantum generative adversarial networks (qGANs) employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel.\n",
    "This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions -- implicitly given by data samples -- into quantum states.\n",
    "Since, this approximate loading requires only $\\mathscr{O}\\left(poly\\left(n\\right)\\right)$ gates and can enable the use of potentially advantageous quantum algorithms by offering an efficient data loading scheme."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import qiskit.tools.jupyter\n",
    "\n",
    "%qiskit_version_table\n",
    "%qiskit_copyright\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}