{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch qGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Description\n",
    "\n",
    "adapted from [PyTorch GAN](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union, List, Optional, Iterable\n",
    "\n",
    "from torch import Tensor, stack, reshape\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import Gradient, StateFn\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.circuit import ParameterExpression, ParameterVector\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.datasets.dataset_helper import discretize_and_truncate\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42\n",
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load training data\n",
    "\n",
    "For testing purposes, we decide for a 2D multivariate normal distribution.\n",
    "Each dimension is represented by 2 qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "data_dim = [3, 3]\n",
    "\n",
    "training_data = algorithm_globals.random.multivariate_normal(mean=[0., 0.],  cov=[[1, 0], [0, 1]], size=1000)\n",
    "# Define minimal and maximal values for the training data\n",
    "bounds_min = np.percentile(training_data, 5, axis=0)\n",
    "bounds_max = np.percentile(training_data, 95, axis=0)\n",
    "bounds = []\n",
    "for i, _ in enumerate(bounds_min):\n",
    "    bounds.append([bounds_min[i], bounds_max[i]])\n",
    "\n",
    "# Pre-processing, i.e., gridding\n",
    "(training_data,\n",
    "data_grid,\n",
    "grid_elements,\n",
    "prob_data ) = discretize_and_truncate(\n",
    "training_data,\n",
    "np.array(bounds),\n",
    "data_dim,\n",
    "return_data_grid_elements=True,\n",
    "return_prob=True,\n",
    "prob_non_zero=True,\n",
    ")\n",
    "\n",
    "# Define the training batch size\n",
    "batch_size = 100\n",
    "dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specify Backend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# declare quantum instance\n",
    "backend = Aer.get_backend('aer_simulator')\n",
    "qi = QuantumInstance(backend, shots = batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definition of quantum generator and the respective gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_(qnn: QuantumCircuit,\n",
    "               parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]]) -> TorchConnector:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "    Returns:\n",
    "        Quantum neural network compatible with PyTorch\n",
    "    \"\"\"\n",
    "    circuit_qnn = CircuitQNN(qnn, input_params=[], weight_params = parameters,\n",
    "                             quantum_instance=qi, sampling=True, sparse=False,\n",
    "                             input_gradients=True, interpret=lambda x: grid_elements[x])\n",
    "    # We use the Qiskit TorchConnector to ensure compatibility with PyTorch\n",
    "    return TorchConnector(circuit_qnn)\n",
    "\n",
    "def generator_grad(qnn: QuantumCircuit,\n",
    "                   parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "                   param_values: Iterable,\n",
    "                   grad_method: str = 'param_shift') -> Iterable:\n",
    "    \"\"\"\n",
    "    Custom generator gradient\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "        grad_method: Method used to compute the gradients {'param_shift', 'lin_comb', 'fin_diff'}\n",
    "    Returns:\n",
    "        List of gradients for the sampling probabilities of the quantum neural network.\n",
    "    \"\"\"\n",
    "    grad = Gradient(grad_method=grad_method).gradient_wrapper(StateFn(qnn), parameters, backend=qi)\n",
    "    grad_values = grad(param_values)\n",
    "    return grad_values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definition of classical discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.Linear_in = nn.Linear(len(data_dim), 50)\n",
    "        self.Leaky_ReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.Linear50 = nn.Linear(50, 20)\n",
    "        self.Linear20 = nn.Linear(20, 1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = self.Linear_in(input)\n",
    "        x = self.Leaky_ReLU(x)\n",
    "        x = self.Linear50(x)\n",
    "        x = self.Leaky_ReLU(x)\n",
    "        x = self.Linear20(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the quantum neural network ansatz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "qnn = QuantumCircuit(sum(data_dim))\n",
    "qnn.h(qnn.qubits)\n",
    "ansatz = TwoLocal(sum(data_dim), \"ry\", \"cx\", reps=2, entanglement=\"circular\")\n",
    "qnn.compose(ansatz, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# discriminator = Discriminator()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# d_params = discriminator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# nn.init.zeros_(d_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definition of the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Loss function\n",
    "g_loss_fun = nn.BCELoss()\n",
    "d_loss_fun = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of custom gradients for the generator BCE loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def g_loss_fun_grad(qnn: QuantumCircuit,\n",
    "                    parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "                    param_values: Iterable,\n",
    "                    discriminator_: nn.Module,\n",
    "                    grad_method: str = 'param_shift') -> Iterable:\n",
    "    \"\"\"\n",
    "    Custom gradient of the generator loss function considering the custom gradients of the quantum generator\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "        discriminator_: Classical neural network representing the discriminator.\n",
    "        grad_method: Method used to compute the gradients {'param_shift', 'lin_comb', 'fin_diff'}\n",
    "    Returns:\n",
    "        List of gradient values, i.e., the gradients of the loss function w.r.t. the quantum neural network parameters\n",
    "    \"\"\"\n",
    "    grads = generator_grad(qnn, parameters, param_values, grad_method = grad_method)\n",
    "    loss_grad = ()\n",
    "    for j, grad in enumerate(grads):\n",
    "        cx = grad[0].tocoo()\n",
    "        input = []\n",
    "        target = []\n",
    "        weight = []\n",
    "        for index, prob_grad in zip(cx.col, cx.data):\n",
    "            input.append(grid_elements[index])\n",
    "            target.append([1.])\n",
    "            weight.append([prob_grad])\n",
    "        bce_loss_grad = F.binary_cross_entropy(discriminator_(Tensor(input)), Tensor(target), weight=Tensor(weight))\n",
    "        loss_grad += (bce_loss_grad, )\n",
    "    loss_grad = stack(loss_grad)\n",
    "    return loss_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the optimizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = generator_(qnn, ansatz.ordered_parameters)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "lr=0.001 #learning rate\n",
    "b1=0.7 #first momentum parameter\n",
    "b2=0.999 #second momentum parameter\n",
    "n_epochs=3 #number of training epochs\n",
    "\n",
    "#optimizer for the generator\n",
    "optimizer_G = Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "#optimizer for the discriminator\n",
    "optimizer_D = Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# TODO discriminator.parameters() move to testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d loss  tensor(0.6953, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.6630, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.62459695 -0.89691746  0.18693757  0.39363968  0.87679446  0.3195883\n",
      "  0.07501841 -0.56848824  0.7865068   0.30438578 -0.02588749 -0.04123104\n",
      "  0.8022851   0.33377373 -0.67609954 -0.55970764  0.96877897 -0.65705943]\n",
      "[Epoch 0/3] [Batch 0/8] [D loss: 0.695323] [G loss: 0.663020]\n",
      "d loss  tensor(0.6907, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.6713, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.623598   -0.8979163   0.1859377   0.3926398   0.87579453  0.3205882\n",
      "  0.07601821 -0.5694881   0.7875065   0.3033869  -0.02688725 -0.04023119\n",
      "  0.80128527  0.33477226 -0.67509973 -0.5587093   0.969777   -0.65605956]\n",
      "[Epoch 0/3] [Batch 1/8] [D loss: 0.690739] [G loss: 0.671256]\n",
      "d loss  tensor(0.6916, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.6807, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.622791   -0.8974322   0.18592344  0.39164877  0.8747792   0.32157525\n",
      "  0.07701366 -0.57005817  0.7884984   0.30250734 -0.02737226 -0.03927406\n",
      "  0.8010951   0.33566642 -0.6740846  -0.55783695  0.9706781  -0.6550562 ]\n",
      "[Epoch 0/3] [Batch 2/8] [D loss: 0.691601] [G loss: 0.680746]\n",
      "d loss  tensor(0.6880, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6226613  -0.89658064  0.18571402  0.3906196   0.87374073  0.3226028\n",
      "  0.0780291  -0.57093376  0.78938055  0.3025077  -0.02677812 -0.03825907\n",
      "  0.8009681   0.3366151  -0.6732863  -0.5571218   0.9716232  -0.65402913]\n",
      "[Epoch 0/3] [Batch 3/8] [D loss: 0.688016] [G loss: 0.686662]\n",
      "d loss  tensor(0.6832, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.6993, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.62186766 -0.8965459   0.18524446  0.39030972  0.87271136  0.3235758\n",
      "  0.07908996 -0.5719291   0.79016906  0.3029567  -0.02605435 -0.03722438\n",
      "  0.80081624  0.3376442  -0.6723268  -0.55619085  0.9726252  -0.6529665 ]\n",
      "[Epoch 0/3] [Batch 4/8] [D loss: 0.683243] [G loss: 0.699334]\n",
      "d loss  tensor(0.6817, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7117, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.62161463 -0.897125    0.18491259  0.39037493  0.8716225   0.3246401\n",
      "  0.08014922 -0.5730034   0.7898764   0.3030924  -0.02521736 -0.03621531\n",
      "  0.8000177   0.3386428  -0.67133    -0.5551496   0.97363794 -0.65188074]\n",
      "[Epoch 0/3] [Batch 5/8] [D loss: 0.681686] [G loss: 0.711715]\n",
      "d loss  tensor(0.6862, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7106, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.621404   -0.8978645   0.1841339   0.39083543  0.870491    0.3257733\n",
      "  0.08127119 -0.5740825   0.7892204   0.30329227 -0.02418871 -0.03513566\n",
      "  0.7991368   0.3393427  -0.6703175  -0.5540343   0.9747446  -0.6508265 ]\n",
      "[Epoch 0/3] [Batch 6/8] [D loss: 0.686197] [G loss: 0.710552]\n",
      "d loss  tensor(0.6827, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6211795  -0.89887875  0.18411495  0.39164028  0.8693939   0.32691273\n",
      "  0.08241753 -0.5752435   0.788439    0.3039538  -0.02304619 -0.03447462\n",
      "  0.79835963  0.33996338 -0.669252   -0.5528557   0.97592086 -0.6497351 ]\n",
      "[Epoch 0/3] [Batch 7/8] [D loss: 0.682705] [G loss: 0.713978]\n",
      "d loss  tensor(0.6715, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7452, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6216256  -0.89964545  0.18384837  0.3924866   0.8682166   0.32813624\n",
      "  0.08359426 -0.5764148   0.787401    0.30491772 -0.0222942  -0.03365706\n",
      "  0.7973605   0.34020072 -0.6689472  -0.5516438   0.9770039  -0.6485895 ]\n",
      "[Epoch 1/3] [Batch 0/8] [D loss: 0.671482] [G loss: 0.745175]\n",
      "d loss  tensor(0.6765, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7342, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.62132    -0.9006552   0.18353751  0.39343825  0.8669991   0.32934934\n",
      "  0.084847   -0.57766104  0.78621113  0.3058513  -0.02155928 -0.03257772\n",
      "  0.79659736  0.3400992  -0.66901875 -0.55040103  0.97805727 -0.6473959 ]\n",
      "[Epoch 1/3] [Batch 1/8] [D loss: 0.676487] [G loss: 0.734168]\n",
      "d loss  tensor(0.6773, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7431, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6207916  -0.90186334  0.18277001  0.39455596  0.86572564  0.33064476\n",
      "  0.08615982 -0.57873553  0.7849083   0.30698863 -0.02054912 -0.03134397\n",
      "  0.7960609   0.34081638 -0.6683985  -0.5491066   0.97895885 -0.6461098 ]\n",
      "[Epoch 1/3] [Batch 2/8] [D loss: 0.677271] [G loss: 0.743064]\n",
      "d loss  tensor(0.6634, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7433, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61983395 -0.9028527   0.1826055   0.39580637  0.8644608   0.33180007\n",
      "  0.08752929 -0.57976985  0.78367454  0.30818352 -0.0197352  -0.0303623\n",
      "  0.79604423  0.34118468 -0.6677363  -0.54774743  0.97978204 -0.64480233]\n",
      "[Epoch 1/3] [Batch 3/8] [D loss: 0.663425] [G loss: 0.743262]\n",
      "d loss  tensor(0.6722, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7552, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61917937 -0.9041095   0.18330957  0.3970019   0.8631311   0.33310995\n",
      "  0.0887963  -0.5807751   0.78240895  0.30930713 -0.0186607  -0.02916485\n",
      "  0.79546934  0.34186018 -0.6673522  -0.5465075   0.98080343 -0.643493  ]\n",
      "[Epoch 1/3] [Batch 4/8] [D loss: 0.672158] [G loss: 0.755179]\n",
      "d loss  tensor(0.6683, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7742, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6180903  -0.9049312   0.18428414  0.3983589   0.86183417  0.33448294\n",
      "  0.09009954 -0.5820091   0.78137773  0.3105033  -0.01738347 -0.02786146\n",
      "  0.7949694   0.34279007 -0.66727793 -0.54523003  0.98183984 -0.6421128 ]\n",
      "[Epoch 1/3] [Batch 5/8] [D loss: 0.668329] [G loss: 0.774217]\n",
      "d loss  tensor(0.6747, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7557, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61694676 -0.90605927  0.1845378   0.39956132  0.8605209   0.33582717\n",
      "  0.0912719  -0.5833529   0.7802695   0.3116296  -0.01636302 -0.02653103\n",
      "  0.79422575  0.34365335 -0.6669103  -0.54384804  0.98304737 -0.64066195]\n",
      "[Epoch 1/3] [Batch 6/8] [D loss: 0.674742] [G loss: 0.755672]\n",
      "d loss  tensor(0.6635, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7675, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61672366 -0.9069956   0.18528003  0.4009349   0.8592265   0.33696043\n",
      "  0.09236551 -0.5847697   0.7788767   0.3129521  -0.01529338 -0.02503924\n",
      "  0.79421955  0.3449369  -0.6660021  -0.5425907   0.9845014  -0.63915545]\n",
      "[Epoch 1/3] [Batch 7/8] [D loss: 0.663470] [G loss: 0.767482]\n",
      "d loss  tensor(0.6761, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7568, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6160615  -0.9080433   0.18602827  0.4022518   0.85786116  0.3382286\n",
      "  0.09356151 -0.586272    0.77745885  0.3132002  -0.01419489 -0.02351638\n",
      "  0.79442865  0.3463419  -0.6647679  -0.541201    0.98593307 -0.63770175]\n",
      "[Epoch 2/3] [Batch 0/8] [D loss: 0.676097] [G loss: 0.756755]\n",
      "d loss  tensor(0.6642, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7862, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6150573  -0.909132    0.18681839  0.40363607  0.85659206  0.3396531\n",
      "  0.09467092 -0.587819    0.77603203  0.31395158 -0.01322337 -0.02196755\n",
      "  0.7937094   0.34607196 -0.6645127  -0.539745    0.98715305 -0.6363376 ]\n",
      "[Epoch 2/3] [Batch 1/8] [D loss: 0.664205] [G loss: 0.786218]\n",
      "d loss  tensor(0.6657, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7865, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6143381  -0.9096877   0.18740721  0.40479532  0.8553304   0.34100118\n",
      "  0.0959866  -0.58923393  0.7745773   0.31467167 -0.01219027 -0.02061021\n",
      "  0.79287636  0.3463064  -0.6638512  -0.53881466  0.9884172  -0.63489616]\n",
      "[Epoch 2/3] [Batch 2/8] [D loss: 0.665659] [G loss: 0.786490]\n",
      "d loss  tensor(0.6790, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7499, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61389863 -0.9106049   0.18855616  0.405817    0.85405123  0.3423761\n",
      "  0.09741166 -0.590686    0.773005    0.3156932  -0.01166708 -0.0193572\n",
      "  0.792719    0.34683084 -0.663374   -0.53786516  0.98978746 -0.6335248 ]\n",
      "[Epoch 2/3] [Batch 3/8] [D loss: 0.678962] [G loss: 0.749932]\n",
      "d loss  tensor(0.6552, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7767, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61345506 -0.9112524   0.18917778  0.4068386   0.85277617  0.34386274\n",
      "  0.098836   -0.59220314  0.7714497   0.31695262 -0.01130846 -0.01800152\n",
      "  0.79259676  0.34740925 -0.662573   -0.5367618   0.991264   -0.63207805]\n",
      "[Epoch 2/3] [Batch 4/8] [D loss: 0.655201] [G loss: 0.776672]\n",
      "d loss  tensor(0.6750, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7735, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6128028  -0.9120592   0.18982795  0.40805864  0.8514267   0.34519604\n",
      "  0.10032255 -0.59354675  0.76976645  0.31770188 -0.01111185 -0.01713809\n",
      "  0.7934951   0.34762403 -0.6617937  -0.5355773   0.9928516  -0.63058215]\n",
      "[Epoch 2/3] [Batch 5/8] [D loss: 0.674995] [G loss: 0.773459]\n",
      "d loss  tensor(0.6557, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7773, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.6120716  -0.9124608   0.19092356  0.4091579   0.849986    0.3465648\n",
      "  0.10188712 -0.5948413   0.7681431   0.31863594 -0.01056974 -0.01660175\n",
      "  0.79423827  0.34688115 -0.6612653  -0.5343422   0.99402374 -0.6291084 ]\n",
      "[Epoch 2/3] [Batch 6/8] [D loss: 0.655654] [G loss: 0.777286]\n",
      "d loss  tensor(0.6536, grad_fn=<DivBackward0>)\n",
      "g loss  tensor(0.7904, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "generator params  [-0.61130846 -0.9129811   0.19165756  0.41046792  0.848499    0.34807074\n",
      "  0.10338581 -0.59606594  0.7665925   0.3199089  -0.01061776 -0.01568867\n",
      "  0.79512256  0.34651956 -0.66038036 -0.5330397   0.9950392  -0.62763834]\n",
      "[Epoch 2/3] [Batch 7/8] [D loss: 0.653618] [G loss: 0.790440]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_data = Variable(data.type(Tensor))\n",
    "        # Generate a batch of images\n",
    "        gen_data = generator()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        disc_data = discriminator(real_data)\n",
    "        real_loss = d_loss_fun(disc_data, valid)\n",
    "        fake_loss = d_loss_fun(discriminator(gen_data), fake)  # (discriminator(gen_data).detach(), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        print('d loss ', d_loss)\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = g_loss_fun(discriminator(gen_data), valid)\n",
    "        g_loss.retain_grad = True\n",
    "        g_loss_grad = g_loss_fun_grad(qnn, ansatz.ordered_parameters, generator.weight.data.numpy(), discriminator)\n",
    "\n",
    "        print('g loss ', g_loss)\n",
    "        print('generator params ',  generator.weight.data.numpy())\n",
    "\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        for j, param in enumerate(generator.parameters()):\n",
    "            param.grad = g_loss_grad\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        # if batches_done % optimizer_G.sample_interval == 0:\n",
    "        #     #TODO: Do something like storing, printing or relative entropy evaluation\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternative approach\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QiskitDevenv",
   "language": "python",
   "name": "qiskitdevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}