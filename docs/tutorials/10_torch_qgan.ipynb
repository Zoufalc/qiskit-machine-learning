{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch qGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "A PyTorch-based Quantum Generative Adversarial Network algorithm.\n",
    "\n",
    "The qGAN \\[1\\] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator $G_{\\theta}$, i.e., an ansatz, and a classical discriminator $D_{\\phi}$, a neural network. to learn the probability distribution underlying given training data.\n",
    "\n",
    "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating samples which the discriminator classifies as training data samples and the discriminator tries to differentiate between training data samples and data samples from the generator. The final goal is for the quantum generator to learn a representation for the training data's underlying probability distribution.\n",
    "The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution.\n",
    "\n",
    "**References:**\n",
    "\n",
    "\\[1\\] Zoufal et al., [Quantum Generative Adversarial Networks for learning and loading random distributions](https://www.nature.com/articles/s41534-019-0223-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Application: qGANs for Loading Random Distributions\n",
    "\n",
    "Given $k$-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data's underlying random distribution and to load it directly into a quantum state:\n",
    "\n",
    "$$ \\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle $$\n",
    "\n",
    "where $p_{\\theta}^{j}$ describe the occurrence probabilities of the basis states $\\big| j\\rangle$.\n",
    "\n",
    "The aim of the qGAN training is to generate a state $\\big| g_{\\theta}\\rangle$ where $p_{\\theta}^{j}$, for $j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}$, describe a probability distribution that is close to the distribution underlying the training data $X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}$.\n",
    "\n",
    "For further details please refer to [Quantum Generative Adversarial Networks for Learning and Loading Random Distributions](https://arxiv.org/abs/1904.00043) _Zoufal, Lucchi, Woerner_ \\[2019\\].\n",
    "\n",
    "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the\n",
    "[Option Pricing with qGANs](https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb) tutorial."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from typing import Union, List, Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterExpression, ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.opflow import Gradient, StateFn\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from scipy.stats import entropy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.datasets.dataset_helper import discretize_and_truncate\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42\n",
    "torch.manual_seed(42)\n",
    "np.random.seed = 42"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data and Representation\n",
    "\n",
    "First, we need to load our training data $X$.\n",
    "\n",
    "In this tutorial, the training data is given by samples from a 2D multivariate normal distribution.\n",
    "\n",
    "The trained generator corresponds to an $n$-qubit quantum state\n",
    "\\begin{equation}\n",
    "|g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle,\n",
    "\\end{equation}\n",
    "where the basis states $|x_{j}\\rangle$ represent the data items in the training data set\n",
    "$X={x_0, \\ldots, x_{k-1}}$ with $k\\leq 2^n$ and $p_j$ refers to the sampling probability\n",
    "of $|x_{j}\\rangle$.\n",
    "\n",
    "To facilitate this representation, we need to map the samples from the multivariate\n",
    "normal distribution to discrete values. The number of values that can be represented\n",
    "depends on the number of qubits used for the mapping.\n",
    "Hence, the data resolution is defined by the number of qubits.\n",
    "If we use $3$ qubits to represent one feature, we have $2^3 = 8$ discrete values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "training_data = np.random.default_rng().multivariate_normal(\n",
    "    mean=[0.0, 0.0], cov=[[1, 0], [0, 1]], size=1000, check_valid=\"warn\", tol=1e-8, method=\"svd\"\n",
    ")\n",
    "# Define minimal and maximal values for the training data\n",
    "bounds_min = np.percentile(training_data, 5, axis=0)\n",
    "bounds_max = np.percentile(training_data, 95, axis=0)\n",
    "bounds = []\n",
    "for i, _ in enumerate(bounds_min):\n",
    "    bounds.append([bounds_min[i], bounds_max[i]])\n",
    "\n",
    "# Determine data resolution for each dimension of the training data in terms\n",
    "# of the number of qubits used to represent each data dimension.\n",
    "data_dim = [3, 3]\n",
    "\n",
    "# Pre-processing, i.e., discretization of the data (gridding)\n",
    "(training_data, _, grid_elements, prob_data) = discretize_and_truncate(\n",
    "    training_data,\n",
    "    np.asarray(bounds),\n",
    "    data_dim,\n",
    "    return_data_grid_elements=True,\n",
    "    return_prob=True,\n",
    "    prob_non_zero=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We convert numpy arrays into tensors and create a data loader from our training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "training_data = torch.tensor(training_data, dtype=torch.float)\n",
    "grid_elements = torch.tensor(grid_elements, dtype=torch.float)\n",
    "\n",
    "# Define the training batch size\n",
    "batch_size = 300\n",
    "dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specify Backend\n",
    "Next, we need to choose a backend that is used to run the quantum generator.\n",
    "The presented method is compatible with all shot-based backends (qasm, fake hardware, real hardware) provided by Qiskit."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "backend = Aer.get_backend(\"aer_simulator\")\n",
    "#Quantum Instance for the training where the batch size defines the number of shots\n",
    "qi = QuantumInstance(backend, shots=batch_size)\n",
    "#Quantum Instance for the evaluation we choose a higher number of shots to get better insights\n",
    "qi_sampling = QuantumInstance(backend, shots=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialization of quantum generator and the respective gradient\n",
    "Next, we define factories that create the quantum generator and a function that calculates the gradients for the quantum generator.\n",
    "Depending on the backend, the latter may be returned in a sparse format. Since PyTorch does not support sparse gradients, we need to write a custom function for gradient based training.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def create_generator(\n",
    "    qnn: QuantumCircuit,\n",
    "    parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "    qinstance: QuantumInstance\n",
    ") -> TorchConnector:\n",
    "    \"\"\"\n",
    "    Factory to create the quantum generator from a given parameterized quantum circuit.\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        qinstance: The quantum instance used to run the quantum generator.\n",
    "    Returns:\n",
    "        Quantum neural network compatible with PyTorch\n",
    "    \"\"\"\n",
    "    circuit_qnn = CircuitQNN(\n",
    "        qnn,\n",
    "        input_params=[],\n",
    "        weight_params=parameters,\n",
    "        quantum_instance=qinstance,\n",
    "        sampling=True,\n",
    "        sparse=False,\n",
    "        input_gradients=True,\n",
    "        interpret=lambda x: grid_elements[x],\n",
    "    )\n",
    "    # We use the Qiskit TorchConnector to ensure compatibility with PyTorch\n",
    "    return TorchConnector(circuit_qnn)\n",
    "\n",
    "\n",
    "def generator_grad(\n",
    "    qnn: QuantumCircuit,\n",
    "    parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "    param_values: Iterable,\n",
    ") -> Iterable:\n",
    "    \"\"\"\n",
    "    The function returns the gradient values for the quantum generator given the\n",
    "    underlying parameterized quantum circuit.\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "    Returns:\n",
    "        List of gradients for the sampling probabilities of the quantum neural network.\n",
    "    \"\"\"\n",
    "    grad = Gradient().gradient_wrapper(StateFn(qnn), parameters, backend=qi)\n",
    "    grad_values = grad(param_values)\n",
    "    return grad_values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of classical discriminator\n",
    "Next, we define factories that create the classical discriminators. The underlying gradients can be automatically computed with PyTorch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class create_discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Factory to create the quantum generator from a given parameterized quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(create_discriminator, self).__init__()\n",
    "\n",
    "        self.Linear_in = nn.Linear(len(data_dim), 20)\n",
    "        self.Leaky_ReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # self.Linear50 = nn.Linear(50, 20)\n",
    "        self.Linear20 = nn.Linear(20, 1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.Linear_in(input)\n",
    "        x = self.Leaky_ReLU(x)\n",
    "        # x = self.Linear50(x)\n",
    "        # x = self.Leaky_ReLU(x)\n",
    "        x = self.Linear20(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the quantum neural network ansatz\n",
    "\n",
    "Now, we are ready to define the parameterized quantum circuit $G\\left(\\boldsymbol{\\theta}\\right)$ with $\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}$ which corresponds to our quantum generator.\n",
    "\n",
    "To implement the quantum generator, we choose a depth-$2$ ansatz that implements $R_Y$ rotations and $CX$ gates which takes a uniform distribution as an input state. Notably, for $k>1$ the generator's parameters must be chosen carefully. For example, the circuit depth should be $>1$ because higher circuit depths enable the representation of more complex structures.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# sum(data_dim) corresponds to the total number of qubits in our quantum circuit (qc)\n",
    "qc = QuantumCircuit(sum(data_dim))\n",
    "qc.h(qc.qubits)\n",
    "# We choose a hardware efficient ansatz.\n",
    "twolocal = TwoLocal(sum(data_dim), \"ry\", \"cx\", reps=2, entanglement=\"circular\")\n",
    "qc.compose(twolocal, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the loss functions\n",
    "We want to train the generator and the discriminator with binary cross entropy as loss function:\n",
    "$$L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],$$\n",
    "where $x_j$ refers to a data sample and $y_j$ to the corresponding label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Generator loss function\n",
    "gen_loss_fun = nn.BCELoss()\n",
    "# Discriminator loss function\n",
    "disc_loss_fun = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of custom gradients for the generator BCE loss function\n",
    "The evaluation of custom gradients for the quantum generator requires us to combine quantum gradients $\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}$ that we compute with Qiskit's gradient framework with the binary cross entropy as follows:\n",
    "$$\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def g_loss_fun_grad(\n",
    "    qnn: QuantumCircuit,\n",
    "    parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "    param_values: Iterable,\n",
    "    discriminator_: nn.Module,\n",
    ") -> Iterable:\n",
    "    \"\"\"\n",
    "    Custom gradient of the generator loss function considering the custom gradients of the quantum generator\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "        discriminator_: Classical neural network representing the discriminator.\n",
    "    Returns:\n",
    "        List of gradient values, i.e., the gradients of the loss function w.r.t. the quantum neural network parameters\n",
    "    \"\"\"\n",
    "    grads = generator_grad(qnn, parameters, param_values)\n",
    "    loss_grad = ()\n",
    "    for j, grad in enumerate(grads):\n",
    "        cx = grad[0].tocoo()\n",
    "        input = torch.zeros(len(cx.col), 2)\n",
    "        target = torch.ones(len(cx.col), 1)\n",
    "        weight = torch.zeros(len(cx.col), 1)\n",
    "\n",
    "        for i, (index, prob_grad) in enumerate(zip(cx.col, cx.data)):\n",
    "            input[i, :] = grid_elements[index]\n",
    "            weight[i, :] = prob_grad\n",
    "        bce_loss_grad = F.binary_cross_entropy(discriminator_(input), target, weight)\n",
    "        loss_grad += (bce_loss_grad,)\n",
    "    loss_grad = torch.stack(loss_grad)\n",
    "    return loss_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relative entropy as benchmarking metric\n",
    "The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def get_relative_entropy(gen_data) -> float:\n",
    "    \"\"\"Get relative entropy between target and trained distribution\"\"\"\n",
    "    prob_gen = np.zeros(len(grid_elements))\n",
    "    for j, item in enumerate(grid_elements):\n",
    "        for gen_item in gen_data.detach().numpy():\n",
    "            if np.allclose(np.round(gen_item, 6), np.round(item, 6), rtol=1e-5):\n",
    "                prob_gen[j] += 1\n",
    "    prob_gen = prob_gen / len(gen_data)\n",
    "    prob_gen = [1e-8 if x == 0 else x for x in prob_gen]\n",
    "    rel_entr = entropy(prob_gen, prob_data)\n",
    "    return rel_entr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the optimizers\n",
    "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see [Kingma et al., Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980) for more details."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = create_generator(qc, twolocal.parameters, qi)\n",
    "discriminator = create_discriminator()\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "b1 = 0.9  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "n_epochs = 500 # number of training epochs\n",
    "\n",
    "# optimizer for the generator\n",
    "optimizer_G = Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "# optimizer for the discriminator\n",
    "optimizer_D = Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Now, we are ready to train our model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/500] [Batch 0/2] [D loss: 0.712499] [G loss: 0.625386]\n",
      "[Epoch 0/500] [Batch 1/2] [D loss: 0.703290] [G loss: 0.661518]\n",
      "[Epoch 1/500] [Batch 0/2] [D loss: 0.690406] [G loss: 0.700945]\n",
      "[Epoch 1/500] [Batch 1/2] [D loss: 0.675671] [G loss: 0.745980]\n",
      "[Epoch 2/500] [Batch 0/2] [D loss: 0.669235] [G loss: 0.778821]\n",
      "[Epoch 2/500] [Batch 1/2] [D loss: 0.661426] [G loss: 0.812518]\n",
      "[Epoch 3/500] [Batch 0/2] [D loss: 0.654749] [G loss: 0.844687]\n",
      "[Epoch 3/500] [Batch 1/2] [D loss: 0.649610] [G loss: 0.863197]\n",
      "[Epoch 4/500] [Batch 0/2] [D loss: 0.640877] [G loss: 0.907358]\n",
      "[Epoch 4/500] [Batch 1/2] [D loss: 0.641178] [G loss: 0.901059]\n",
      "[Epoch 5/500] [Batch 0/2] [D loss: 0.642870] [G loss: 0.916974]\n",
      "[Epoch 5/500] [Batch 1/2] [D loss: 0.637858] [G loss: 0.919393]\n",
      "[Epoch 6/500] [Batch 0/2] [D loss: 0.643898] [G loss: 0.917480]\n",
      "[Epoch 6/500] [Batch 1/2] [D loss: 0.628998] [G loss: 0.945367]\n",
      "[Epoch 7/500] [Batch 0/2] [D loss: 0.634511] [G loss: 0.954124]\n",
      "[Epoch 7/500] [Batch 1/2] [D loss: 0.636859] [G loss: 0.925632]\n",
      "[Epoch 8/500] [Batch 0/2] [D loss: 0.656494] [G loss: 0.864707]\n",
      "[Epoch 8/500] [Batch 1/2] [D loss: 0.660112] [G loss: 0.866728]\n",
      "[Epoch 9/500] [Batch 0/2] [D loss: 0.660632] [G loss: 0.877036]\n",
      "[Epoch 9/500] [Batch 1/2] [D loss: 0.646302] [G loss: 0.860519]\n",
      "[Epoch 10/500] [Batch 0/2] [D loss: 0.663188] [G loss: 0.828113]\n",
      "[Epoch 10/500] [Batch 1/2] [D loss: 0.665352] [G loss: 0.817377]\n",
      "[Epoch 11/500] [Batch 0/2] [D loss: 0.681341] [G loss: 0.799114]\n",
      "[Epoch 11/500] [Batch 1/2] [D loss: 0.669895] [G loss: 0.806138]\n",
      "[Epoch 12/500] [Batch 0/2] [D loss: 0.653822] [G loss: 0.795587]\n",
      "[Epoch 12/500] [Batch 1/2] [D loss: 0.683722] [G loss: 0.802384]\n",
      "[Epoch 13/500] [Batch 0/2] [D loss: 0.681100] [G loss: 0.778060]\n",
      "[Epoch 13/500] [Batch 1/2] [D loss: 0.682265] [G loss: 0.767249]\n",
      "[Epoch 14/500] [Batch 0/2] [D loss: 0.711511] [G loss: 0.730041]\n",
      "[Epoch 14/500] [Batch 1/2] [D loss: 0.697936] [G loss: 0.749218]\n",
      "[Epoch 15/500] [Batch 0/2] [D loss: 0.696258] [G loss: 0.745769]\n",
      "[Epoch 15/500] [Batch 1/2] [D loss: 0.715170] [G loss: 0.729277]\n",
      "[Epoch 16/500] [Batch 0/2] [D loss: 0.708618] [G loss: 0.696262]\n",
      "[Epoch 16/500] [Batch 1/2] [D loss: 0.691223] [G loss: 0.738340]\n",
      "[Epoch 17/500] [Batch 0/2] [D loss: 0.694831] [G loss: 0.708385]\n",
      "[Epoch 17/500] [Batch 1/2] [D loss: 0.724400] [G loss: 0.698541]\n",
      "[Epoch 18/500] [Batch 0/2] [D loss: 0.722363] [G loss: 0.696305]\n",
      "[Epoch 18/500] [Batch 1/2] [D loss: 0.715164] [G loss: 0.698885]\n",
      "[Epoch 19/500] [Batch 0/2] [D loss: 0.721227] [G loss: 0.701813]\n",
      "[Epoch 19/500] [Batch 1/2] [D loss: 0.709731] [G loss: 0.707480]\n",
      "[Epoch 20/500] [Batch 0/2] [D loss: 0.709157] [G loss: 0.696433]\n",
      "[Epoch 20/500] [Batch 1/2] [D loss: 0.717614] [G loss: 0.696529]\n",
      "[Epoch 21/500] [Batch 0/2] [D loss: 0.707680] [G loss: 0.704755]\n",
      "[Epoch 21/500] [Batch 1/2] [D loss: 0.713105] [G loss: 0.695415]\n",
      "[Epoch 22/500] [Batch 0/2] [D loss: 0.715963] [G loss: 0.677372]\n",
      "[Epoch 22/500] [Batch 1/2] [D loss: 0.705562] [G loss: 0.693043]\n",
      "[Epoch 23/500] [Batch 0/2] [D loss: 0.697518] [G loss: 0.704836]\n",
      "[Epoch 23/500] [Batch 1/2] [D loss: 0.704606] [G loss: 0.689616]\n",
      "[Epoch 24/500] [Batch 0/2] [D loss: 0.700375] [G loss: 0.704147]\n",
      "[Epoch 24/500] [Batch 1/2] [D loss: 0.696107] [G loss: 0.704577]\n",
      "[Epoch 25/500] [Batch 0/2] [D loss: 0.700432] [G loss: 0.704921]\n",
      "[Epoch 25/500] [Batch 1/2] [D loss: 0.693368] [G loss: 0.709769]\n",
      "[Epoch 26/500] [Batch 0/2] [D loss: 0.691818] [G loss: 0.714097]\n",
      "[Epoch 26/500] [Batch 1/2] [D loss: 0.691283] [G loss: 0.718257]\n",
      "[Epoch 27/500] [Batch 0/2] [D loss: 0.691288] [G loss: 0.717459]\n",
      "[Epoch 27/500] [Batch 1/2] [D loss: 0.687730] [G loss: 0.718801]\n",
      "[Epoch 28/500] [Batch 0/2] [D loss: 0.686017] [G loss: 0.725250]\n",
      "[Epoch 28/500] [Batch 1/2] [D loss: 0.685567] [G loss: 0.720236]\n",
      "[Epoch 29/500] [Batch 0/2] [D loss: 0.678769] [G loss: 0.734704]\n",
      "[Epoch 29/500] [Batch 1/2] [D loss: 0.686171] [G loss: 0.730466]\n",
      "[Epoch 30/500] [Batch 0/2] [D loss: 0.683452] [G loss: 0.733928]\n",
      "[Epoch 30/500] [Batch 1/2] [D loss: 0.680553] [G loss: 0.736207]\n",
      "[Epoch 31/500] [Batch 0/2] [D loss: 0.680964] [G loss: 0.740725]\n",
      "[Epoch 31/500] [Batch 1/2] [D loss: 0.677382] [G loss: 0.747360]\n",
      "[Epoch 32/500] [Batch 0/2] [D loss: 0.681850] [G loss: 0.729525]\n",
      "[Epoch 32/500] [Batch 1/2] [D loss: 0.678180] [G loss: 0.734372]\n",
      "[Epoch 33/500] [Batch 0/2] [D loss: 0.677273] [G loss: 0.742477]\n",
      "[Epoch 33/500] [Batch 1/2] [D loss: 0.679458] [G loss: 0.741600]\n",
      "[Epoch 34/500] [Batch 0/2] [D loss: 0.686627] [G loss: 0.714337]\n",
      "[Epoch 34/500] [Batch 1/2] [D loss: 0.685671] [G loss: 0.720292]\n",
      "[Epoch 35/500] [Batch 0/2] [D loss: 0.677760] [G loss: 0.743892]\n",
      "[Epoch 35/500] [Batch 1/2] [D loss: 0.663411] [G loss: 0.749997]\n",
      "[Epoch 36/500] [Batch 0/2] [D loss: 0.672596] [G loss: 0.728622]\n",
      "[Epoch 36/500] [Batch 1/2] [D loss: 0.687876] [G loss: 0.717592]\n",
      "[Epoch 37/500] [Batch 0/2] [D loss: 0.691630] [G loss: 0.728444]\n",
      "[Epoch 37/500] [Batch 1/2] [D loss: 0.666820] [G loss: 0.716004]\n",
      "[Epoch 38/500] [Batch 0/2] [D loss: 0.688078] [G loss: 0.715927]\n",
      "[Epoch 38/500] [Batch 1/2] [D loss: 0.686307] [G loss: 0.703913]\n",
      "[Epoch 39/500] [Batch 0/2] [D loss: 0.699902] [G loss: 0.689571]\n",
      "[Epoch 39/500] [Batch 1/2] [D loss: 0.681280] [G loss: 0.709519]\n",
      "[Epoch 40/500] [Batch 0/2] [D loss: 0.684744] [G loss: 0.712280]\n",
      "[Epoch 40/500] [Batch 1/2] [D loss: 0.691168] [G loss: 0.713576]\n",
      "[Epoch 41/500] [Batch 0/2] [D loss: 0.685086] [G loss: 0.714172]\n",
      "[Epoch 41/500] [Batch 1/2] [D loss: 0.679453] [G loss: 0.726661]\n",
      "[Epoch 42/500] [Batch 0/2] [D loss: 0.697977] [G loss: 0.705103]\n",
      "[Epoch 42/500] [Batch 1/2] [D loss: 0.683032] [G loss: 0.722638]\n",
      "[Epoch 43/500] [Batch 0/2] [D loss: 0.683165] [G loss: 0.732231]\n",
      "[Epoch 43/500] [Batch 1/2] [D loss: 0.683536] [G loss: 0.731488]\n",
      "[Epoch 44/500] [Batch 0/2] [D loss: 0.691637] [G loss: 0.734699]\n",
      "[Epoch 44/500] [Batch 1/2] [D loss: 0.681023] [G loss: 0.737984]\n",
      "[Epoch 45/500] [Batch 0/2] [D loss: 0.684295] [G loss: 0.742776]\n",
      "[Epoch 45/500] [Batch 1/2] [D loss: 0.684346] [G loss: 0.742343]\n",
      "[Epoch 46/500] [Batch 0/2] [D loss: 0.678112] [G loss: 0.745147]\n",
      "[Epoch 46/500] [Batch 1/2] [D loss: 0.681729] [G loss: 0.747795]\n",
      "[Epoch 47/500] [Batch 0/2] [D loss: 0.680448] [G loss: 0.750978]\n",
      "[Epoch 47/500] [Batch 1/2] [D loss: 0.677896] [G loss: 0.752187]\n",
      "[Epoch 48/500] [Batch 0/2] [D loss: 0.684191] [G loss: 0.739803]\n",
      "[Epoch 48/500] [Batch 1/2] [D loss: 0.672637] [G loss: 0.753367]\n",
      "[Epoch 49/500] [Batch 0/2] [D loss: 0.673137] [G loss: 0.747686]\n",
      "[Epoch 49/500] [Batch 1/2] [D loss: 0.680354] [G loss: 0.732832]\n",
      "[Epoch 50/500] [Batch 0/2] [D loss: 0.669350] [G loss: 0.742959]\n",
      "[Epoch 50/500] [Batch 1/2] [D loss: 0.674513] [G loss: 0.735187]\n",
      "[Epoch 51/500] [Batch 0/2] [D loss: 0.667471] [G loss: 0.745012]\n",
      "[Epoch 51/500] [Batch 1/2] [D loss: 0.672444] [G loss: 0.725351]\n",
      "[Epoch 52/500] [Batch 0/2] [D loss: 0.671579] [G loss: 0.725910]\n",
      "[Epoch 52/500] [Batch 1/2] [D loss: 0.674455] [G loss: 0.725497]\n",
      "[Epoch 53/500] [Batch 0/2] [D loss: 0.664560] [G loss: 0.744261]\n",
      "[Epoch 53/500] [Batch 1/2] [D loss: 0.671675] [G loss: 0.734173]\n",
      "[Epoch 54/500] [Batch 0/2] [D loss: 0.664824] [G loss: 0.739249]\n",
      "[Epoch 54/500] [Batch 1/2] [D loss: 0.681764] [G loss: 0.713781]\n",
      "[Epoch 55/500] [Batch 0/2] [D loss: 0.671516] [G loss: 0.740710]\n",
      "[Epoch 55/500] [Batch 1/2] [D loss: 0.668182] [G loss: 0.748490]\n",
      "[Epoch 56/500] [Batch 0/2] [D loss: 0.662779] [G loss: 0.755209]\n",
      "[Epoch 56/500] [Batch 1/2] [D loss: 0.665741] [G loss: 0.757788]\n",
      "[Epoch 57/500] [Batch 0/2] [D loss: 0.676443] [G loss: 0.738810]\n",
      "[Epoch 57/500] [Batch 1/2] [D loss: 0.667531] [G loss: 0.748900]\n",
      "[Epoch 58/500] [Batch 0/2] [D loss: 0.680693] [G loss: 0.731971]\n",
      "[Epoch 58/500] [Batch 1/2] [D loss: 0.669336] [G loss: 0.741991]\n",
      "[Epoch 59/500] [Batch 0/2] [D loss: 0.675334] [G loss: 0.732821]\n"
     ]
    }
   ],
   "source": [
    "# Relative entropy list\n",
    "rel_entr_list = []\n",
    "# Generator loss list\n",
    "g_loss_list = []\n",
    "# Discriminator loss list\n",
    "d_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    rel_entr = []\n",
    "    g_loss_ = []\n",
    "    d_loss_ = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        # valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        valid = torch.ones(data.size(0), 1)\n",
    "        # fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "        fake = torch.zeros(data.size(0), 1)\n",
    "\n",
    "        # Configure input\n",
    "        # real_data = Variable(data.type(Tensor))\n",
    "        real_data = data\n",
    "        # Generate a batch of data points\n",
    "        gen_data = generator()\n",
    "\n",
    "        # Evaluate Relative Entropy\n",
    "        rel_entr.append(get_relative_entropy(gen_data))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss measures discriminator's ability to distinguish real from generated samples\n",
    "        disc_data = discriminator(real_data)\n",
    "        real_loss = disc_loss_fun(disc_data, valid)\n",
    "        fake_loss = disc_loss_fun(discriminator(gen_data), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Loss measures generator's ability to prepare good data samples\n",
    "        g_loss = gen_loss_fun(discriminator(gen_data), valid)\n",
    "        g_loss.retain_grad = True\n",
    "        g_loss_grad = g_loss_fun_grad(\n",
    "            qc, twolocal.ordered_parameters, generator.weight.data.numpy(), discriminator\n",
    "        )\n",
    "\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        for j, param in enumerate(generator.parameters()):\n",
    "            param.grad = g_loss_grad\n",
    "        optimizer_G.step()\n",
    "\n",
    "        g_loss_.append(g_loss.item())\n",
    "        d_loss_.append(d_loss.item())\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "    rel_entr_list.append(np.mean(rel_entr))\n",
    "    g_loss_list.append(np.mean(g_loss_))\n",
    "    d_loss_list.append(np.mean(d_loss_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plots\n",
    "Let's visualize what happened during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution.\n",
    "\n",
    "Finally, we also compare the probability density function (PDF) of the trained distribution to the PDF of the target distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Relative Entropy\n",
    "plt.figure()\n",
    "plt.title(\"Relative entropy\")\n",
    "plt.plot(rel_entr_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Relative entropy\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(g_loss_list, label=\"generator loss\", color=\"blue\")\n",
    "plt.plot(d_loss_list, label=\"discriminator loss\", color=\"magenta\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a generator for sampling which takes more shots and, hence, gives more information\n",
    "generator_sampling = create_generator(qc, twolocal.parameters, qi_sampling)\n",
    "generator_sampling.weight.data = generator.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PDF\n",
    "# Get the generator data samples and corresponding sampling probabilities\n",
    "gen_data = generator_sampling().detach().numpy()\n",
    "prob_gen = np.zeros(len(grid_elements))\n",
    "for j, item in enumerate(grid_elements):\n",
    "    for gen_item in gen_data:\n",
    "        if np.allclose(np.round(gen_item, 6), np.round(item, 6), rtol=1e-5):\n",
    "            prob_gen[j] += 1\n",
    "prob_gen = prob_gen / len(gen_data)\n",
    "\n",
    "prob_gen = [1e-8 if x == 0 else x for x in prob_gen]\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "ax1.set_title(\"PDF\")\n",
    "# TODO add legend - known issue with matplotlib unresolved\n",
    "ax1.bar3d(\n",
    "    np.transpose(grid_elements)[0],\n",
    "    np.transpose(grid_elements)[1],\n",
    "    prob_gen,\n",
    "    0.1,\n",
    "    0.1,\n",
    "    prob_gen,\n",
    "    label=\"generated data\",\n",
    "    color=\"royalblue\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax1.bar3d(\n",
    "    np.transpose(grid_elements)[0],\n",
    "    np.transpose(grid_elements)[1],\n",
    "    prob_data,\n",
    "    0.1,\n",
    "    0.1,\n",
    "    prob_data,\n",
    "    label=\"training data\",\n",
    "    color=\"deepskyblue\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "# ax1.legend(loc='upper right')\n",
    "ax1.set_xlabel(\"x_0\")\n",
    "ax1.set_ylabel(\"x_1\")\n",
    "ax1.set_zlabel(\"p(x)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import qiskit.tools.jupyter\n",
    "\n",
    "%qiskit_version_table\n",
    "%qiskit_copyright\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "qiskitdevenv",
   "language": "python",
   "display_name": "QiskitDevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}