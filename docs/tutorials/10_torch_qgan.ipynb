{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch qGAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Description\n",
    "\n",
    "adapted from [PyTorch GAN](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fe7909fa310>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from typing import Union, List, Optional, Iterable\n",
    "\n",
    "from torch import Tensor, stack, reshape, manual_seed, log, clamp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import Gradient, StateFn\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.circuit import ParameterExpression, ParameterVector\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.datasets.dataset_helper import discretize_and_truncate\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42\n",
    "np.random.seed = 42\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load training data\n",
    "\n",
    "For testing purposes, we decide for a 2D multivariate normal distribution.\n",
    "Each dimension is represented by 2 qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "data_dim = [3, 3]\n",
    "\n",
    "training_data = np.random.default_rng().multivariate_normal(mean=[0., 0.], cov=[[1, 0], [0, 1]], size=1000, check_valid='warn',\n",
    "                                                        tol=1e-8, method='svd')\n",
    "# Define minimal and maximal values for the training data\n",
    "bounds_min = np.percentile(training_data, 5, axis=0)\n",
    "bounds_max = np.percentile(training_data, 95, axis=0)\n",
    "bounds = []\n",
    "for i, _ in enumerate(bounds_min):\n",
    "    bounds.append([bounds_min[i], bounds_max[i]])\n",
    "\n",
    "# Pre-processing, i.e., gridding\n",
    "(training_data,\n",
    "data_grid,\n",
    "grid_elements,\n",
    "prob_data ) = discretize_and_truncate(\n",
    "training_data,\n",
    "np.array(bounds),\n",
    "data_dim,\n",
    "return_data_grid_elements=True,\n",
    "return_prob=True,\n",
    "prob_non_zero=True,\n",
    ")\n",
    "\n",
    "# Define the training batch size\n",
    "batch_size = 300\n",
    "dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specify Backend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# declare quantum instance\n",
    "backend = Aer.get_backend('aer_simulator')\n",
    "qi = QuantumInstance(backend, shots = batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definition of quantum generator and the respective gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator_(qnn: QuantumCircuit,\n",
    "               parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]]) -> TorchConnector:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "    Returns:\n",
    "        Quantum neural network compatible with PyTorch\n",
    "    \"\"\"\n",
    "    circuit_qnn = CircuitQNN(qnn, input_params=[], weight_params = parameters,\n",
    "                             quantum_instance=qi, sampling=True, sparse=False,\n",
    "                             input_gradients=True, interpret=lambda x: grid_elements[x])\n",
    "    # We use the Qiskit TorchConnector to ensure compatibility with PyTorch\n",
    "    return TorchConnector(circuit_qnn)\n",
    "\n",
    "def generator_grad(qnn: QuantumCircuit,\n",
    "                   parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "                   param_values: Iterable,\n",
    "                   grad_method: str = 'param_shift') -> Iterable:\n",
    "    \"\"\"\n",
    "    Custom generator gradient\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "        grad_method: Method used to compute the gradients {'param_shift', 'lin_comb', 'fin_diff'}\n",
    "    Returns:\n",
    "        List of gradients for the sampling probabilities of the quantum neural network.\n",
    "    \"\"\"\n",
    "    grad = Gradient(grad_method=grad_method).gradient_wrapper(StateFn(qnn), parameters, backend=qi)\n",
    "    grad_values = grad(param_values)\n",
    "    return grad_values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definition of classical discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.Linear_in = nn.Linear(len(data_dim), 20)\n",
    "        self.Leaky_ReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # self.Linear50 = nn.Linear(50, 20)\n",
    "        self.Linear20 = nn.Linear(20, 1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = self.Linear_in(input)\n",
    "        x = self.Leaky_ReLU(x)\n",
    "        # x = self.Linear50(x)\n",
    "        # x = self.Leaky_ReLU(x)\n",
    "        x = self.Linear20(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the quantum neural network ansatz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "qnn = QuantumCircuit(sum(data_dim))\n",
    "qnn.h(qnn.qubits)\n",
    "ansatz = TwoLocal(sum(data_dim), \"ry\", \"cx\", reps=2, entanglement=\"circular\")\n",
    "qnn.compose(ansatz, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# discriminator = Discriminator()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# d_params = discriminator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "# nn.init.zeros_(d_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the loss functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# Loss function\n",
    "g_loss_fun = nn.BCELoss()\n",
    "d_loss_fun = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of custom gradients for the generator BCE loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def g_loss_fun_grad(qnn: QuantumCircuit,\n",
    "                    parameters: Union[ParameterVector, ParameterExpression, List[ParameterExpression]],\n",
    "                    param_values: Iterable,\n",
    "                    discriminator_: nn.Module,\n",
    "                    grad_method: str = 'param_shift') -> Iterable:\n",
    "    \"\"\"\n",
    "    Custom gradient of the generator loss function considering the custom gradients of the quantum generator\n",
    "    Args:\n",
    "        qnn: Quantum neural network ansatz given as a quantum circuit.\n",
    "        parameters: The parameters of the quantum neural network which are trained.\n",
    "        param_values: The current values of the quantum neural network parameters.\n",
    "        discriminator_: Classical neural network representing the discriminator.\n",
    "        grad_method: Method used to compute the gradients {'param_shift', 'lin_comb', 'fin_diff'}\n",
    "    Returns:\n",
    "        List of gradient values, i.e., the gradients of the loss function w.r.t. the quantum neural network parameters\n",
    "    \"\"\"\n",
    "    grads = generator_grad(qnn, parameters, param_values, grad_method = grad_method)\n",
    "    loss_grad = ()\n",
    "    for j, grad in enumerate(grads):\n",
    "        cx = grad[0].tocoo()\n",
    "        input = []\n",
    "        target = []\n",
    "        weight = []\n",
    "        for index, prob_grad in zip(cx.col, cx.data):\n",
    "            input.append(grid_elements[index])\n",
    "            target.append([1.])\n",
    "            weight.append([prob_grad])\n",
    "        bce_loss_grad = F.binary_cross_entropy(discriminator_(Tensor(input)), Tensor(target), weight=Tensor(weight))\n",
    "        loss_grad += (bce_loss_grad, )\n",
    "    loss_grad = stack(loss_grad)\n",
    "    return loss_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of relative entropy as benchmarking metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def get_rel_entr(gen_data) -> float:\n",
    "    \"\"\"Get relative entropy between target and trained distribution\"\"\"\n",
    "    prob_gen = np.zeros(len(grid_elements))\n",
    "    for j, item in enumerate(grid_elements):\n",
    "        for gen_item in gen_data.detach().numpy():\n",
    "            if np.allclose(np.round(gen_item, 6), np.round(item, 6), rtol=1e-5):\n",
    "                prob_gen[j] += 1\n",
    "    prob_gen=prob_gen/len(gen_data)\n",
    "    prob_gen = [1e-8 if x == 0 else x for x in prob_gen]\n",
    "    rel_entr = entropy(prob_gen, prob_data)\n",
    "    return rel_entr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the optimizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = generator_(qnn, ansatz.parameters)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "lr=0.01 #learning rate\n",
    "b1=0.9 #first momentum parameter\n",
    "b2=0.999 #second momentum parameter\n",
    "n_epochs=100 #number of training epochs\n",
    "\n",
    "#optimizer for the generator\n",
    "optimizer_G = Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "#optimizer for the discriminator\n",
    "optimizer_D = Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# TODO discriminator.parameters() move to testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/2] [D loss: 0.714448] [G loss: 0.928033]\n",
      "[Epoch 0/100] [Batch 1/2] [D loss: 0.706379] [G loss: 0.900403]\n",
      "[Epoch 1/100] [Batch 0/2] [D loss: 0.701542] [G loss: 0.860606]\n",
      "[Epoch 1/100] [Batch 1/2] [D loss: 0.695915] [G loss: 0.829850]\n",
      "[Epoch 2/100] [Batch 0/2] [D loss: 0.695938] [G loss: 0.792841]\n",
      "[Epoch 2/100] [Batch 1/2] [D loss: 0.683551] [G loss: 0.785370]\n",
      "[Epoch 3/100] [Batch 0/2] [D loss: 0.687643] [G loss: 0.749159]\n",
      "[Epoch 3/100] [Batch 1/2] [D loss: 0.688523] [G loss: 0.718854]\n",
      "[Epoch 4/100] [Batch 0/2] [D loss: 0.689570] [G loss: 0.693075]\n",
      "[Epoch 4/100] [Batch 1/2] [D loss: 0.690001] [G loss: 0.683160]\n",
      "[Epoch 5/100] [Batch 0/2] [D loss: 0.676524] [G loss: 0.690667]\n",
      "[Epoch 5/100] [Batch 1/2] [D loss: 0.688309] [G loss: 0.662591]\n",
      "[Epoch 6/100] [Batch 0/2] [D loss: 0.690404] [G loss: 0.649922]\n",
      "[Epoch 6/100] [Batch 1/2] [D loss: 0.678711] [G loss: 0.664482]\n",
      "[Epoch 7/100] [Batch 0/2] [D loss: 0.677886] [G loss: 0.664639]\n",
      "[Epoch 7/100] [Batch 1/2] [D loss: 0.677033] [G loss: 0.673912]\n",
      "[Epoch 8/100] [Batch 0/2] [D loss: 0.678217] [G loss: 0.671936]\n",
      "[Epoch 8/100] [Batch 1/2] [D loss: 0.675633] [G loss: 0.685951]\n",
      "[Epoch 9/100] [Batch 0/2] [D loss: 0.677042] [G loss: 0.690175]\n",
      "[Epoch 9/100] [Batch 1/2] [D loss: 0.671922] [G loss: 0.712925]\n",
      "[Epoch 10/100] [Batch 0/2] [D loss: 0.671638] [G loss: 0.729222]\n",
      "[Epoch 10/100] [Batch 1/2] [D loss: 0.662907] [G loss: 0.745781]\n",
      "[Epoch 11/100] [Batch 0/2] [D loss: 0.668260] [G loss: 0.751715]\n",
      "[Epoch 11/100] [Batch 1/2] [D loss: 0.661456] [G loss: 0.770666]\n",
      "[Epoch 12/100] [Batch 0/2] [D loss: 0.666179] [G loss: 0.777727]\n",
      "[Epoch 12/100] [Batch 1/2] [D loss: 0.672484] [G loss: 0.794270]\n",
      "[Epoch 13/100] [Batch 0/2] [D loss: 0.664001] [G loss: 0.805530]\n",
      "[Epoch 13/100] [Batch 1/2] [D loss: 0.671735] [G loss: 0.813847]\n",
      "[Epoch 14/100] [Batch 0/2] [D loss: 0.666787] [G loss: 0.810311]\n",
      "[Epoch 14/100] [Batch 1/2] [D loss: 0.666722] [G loss: 0.831519]\n",
      "[Epoch 15/100] [Batch 0/2] [D loss: 0.669312] [G loss: 0.806530]\n",
      "[Epoch 15/100] [Batch 1/2] [D loss: 0.671910] [G loss: 0.820545]\n",
      "[Epoch 16/100] [Batch 0/2] [D loss: 0.663078] [G loss: 0.823215]\n",
      "[Epoch 16/100] [Batch 1/2] [D loss: 0.661199] [G loss: 0.839106]\n",
      "[Epoch 17/100] [Batch 0/2] [D loss: 0.667548] [G loss: 0.799133]\n",
      "[Epoch 17/100] [Batch 1/2] [D loss: 0.664099] [G loss: 0.806629]\n",
      "[Epoch 18/100] [Batch 0/2] [D loss: 0.664559] [G loss: 0.795514]\n",
      "[Epoch 18/100] [Batch 1/2] [D loss: 0.669559] [G loss: 0.792469]\n",
      "[Epoch 19/100] [Batch 0/2] [D loss: 0.671909] [G loss: 0.758013]\n",
      "[Epoch 19/100] [Batch 1/2] [D loss: 0.688414] [G loss: 0.749683]\n",
      "[Epoch 20/100] [Batch 0/2] [D loss: 0.662899] [G loss: 0.767273]\n",
      "[Epoch 20/100] [Batch 1/2] [D loss: 0.669652] [G loss: 0.750738]\n",
      "[Epoch 21/100] [Batch 0/2] [D loss: 0.676337] [G loss: 0.727760]\n",
      "[Epoch 21/100] [Batch 1/2] [D loss: 0.666139] [G loss: 0.731677]\n",
      "[Epoch 22/100] [Batch 0/2] [D loss: 0.665473] [G loss: 0.736413]\n",
      "[Epoch 22/100] [Batch 1/2] [D loss: 0.681852] [G loss: 0.711277]\n",
      "[Epoch 23/100] [Batch 0/2] [D loss: 0.672650] [G loss: 0.711779]\n",
      "[Epoch 23/100] [Batch 1/2] [D loss: 0.675046] [G loss: 0.737758]\n",
      "[Epoch 24/100] [Batch 0/2] [D loss: 0.690387] [G loss: 0.696248]\n",
      "[Epoch 24/100] [Batch 1/2] [D loss: 0.684180] [G loss: 0.703097]\n",
      "[Epoch 25/100] [Batch 0/2] [D loss: 0.686902] [G loss: 0.701096]\n",
      "[Epoch 25/100] [Batch 1/2] [D loss: 0.684877] [G loss: 0.712513]\n",
      "[Epoch 26/100] [Batch 0/2] [D loss: 0.688548] [G loss: 0.703441]\n",
      "[Epoch 26/100] [Batch 1/2] [D loss: 0.684270] [G loss: 0.697455]\n",
      "[Epoch 27/100] [Batch 0/2] [D loss: 0.701279] [G loss: 0.685016]\n",
      "[Epoch 27/100] [Batch 1/2] [D loss: 0.685187] [G loss: 0.695802]\n",
      "[Epoch 28/100] [Batch 0/2] [D loss: 0.683460] [G loss: 0.699782]\n",
      "[Epoch 28/100] [Batch 1/2] [D loss: 0.694517] [G loss: 0.679397]\n",
      "[Epoch 29/100] [Batch 0/2] [D loss: 0.691943] [G loss: 0.688346]\n",
      "[Epoch 29/100] [Batch 1/2] [D loss: 0.690558] [G loss: 0.683241]\n",
      "[Epoch 30/100] [Batch 0/2] [D loss: 0.682314] [G loss: 0.679904]\n",
      "[Epoch 30/100] [Batch 1/2] [D loss: 0.690422] [G loss: 0.679443]\n",
      "[Epoch 31/100] [Batch 0/2] [D loss: 0.688878] [G loss: 0.686784]\n",
      "[Epoch 31/100] [Batch 1/2] [D loss: 0.692197] [G loss: 0.676959]\n",
      "[Epoch 32/100] [Batch 0/2] [D loss: 0.694098] [G loss: 0.681747]\n",
      "[Epoch 32/100] [Batch 1/2] [D loss: 0.694925] [G loss: 0.680913]\n",
      "[Epoch 33/100] [Batch 0/2] [D loss: 0.692212] [G loss: 0.690196]\n",
      "[Epoch 33/100] [Batch 1/2] [D loss: 0.697158] [G loss: 0.691082]\n",
      "[Epoch 34/100] [Batch 0/2] [D loss: 0.700474] [G loss: 0.701054]\n",
      "[Epoch 34/100] [Batch 1/2] [D loss: 0.690478] [G loss: 0.711281]\n",
      "[Epoch 35/100] [Batch 0/2] [D loss: 0.691510] [G loss: 0.728924]\n",
      "[Epoch 35/100] [Batch 1/2] [D loss: 0.697904] [G loss: 0.720582]\n",
      "[Epoch 36/100] [Batch 0/2] [D loss: 0.692157] [G loss: 0.725893]\n",
      "[Epoch 36/100] [Batch 1/2] [D loss: 0.689483] [G loss: 0.744325]\n",
      "[Epoch 37/100] [Batch 0/2] [D loss: 0.687412] [G loss: 0.750572]\n",
      "[Epoch 37/100] [Batch 1/2] [D loss: 0.690149] [G loss: 0.739048]\n",
      "[Epoch 38/100] [Batch 0/2] [D loss: 0.688380] [G loss: 0.743626]\n",
      "[Epoch 38/100] [Batch 1/2] [D loss: 0.691370] [G loss: 0.732717]\n",
      "[Epoch 39/100] [Batch 0/2] [D loss: 0.688579] [G loss: 0.742730]\n",
      "[Epoch 39/100] [Batch 1/2] [D loss: 0.694070] [G loss: 0.716648]\n",
      "[Epoch 40/100] [Batch 0/2] [D loss: 0.688440] [G loss: 0.712161]\n",
      "[Epoch 40/100] [Batch 1/2] [D loss: 0.690843] [G loss: 0.702349]\n",
      "[Epoch 41/100] [Batch 0/2] [D loss: 0.685583] [G loss: 0.706791]\n",
      "[Epoch 41/100] [Batch 1/2] [D loss: 0.682698] [G loss: 0.693980]\n",
      "[Epoch 42/100] [Batch 0/2] [D loss: 0.682985] [G loss: 0.693074]\n",
      "[Epoch 42/100] [Batch 1/2] [D loss: 0.680294] [G loss: 0.693921]\n",
      "[Epoch 43/100] [Batch 0/2] [D loss: 0.688535] [G loss: 0.683203]\n",
      "[Epoch 43/100] [Batch 1/2] [D loss: 0.689826] [G loss: 0.684636]\n",
      "[Epoch 44/100] [Batch 0/2] [D loss: 0.680214] [G loss: 0.703173]\n",
      "[Epoch 44/100] [Batch 1/2] [D loss: 0.685909] [G loss: 0.691711]\n",
      "[Epoch 45/100] [Batch 0/2] [D loss: 0.685305] [G loss: 0.697257]\n",
      "[Epoch 45/100] [Batch 1/2] [D loss: 0.687076] [G loss: 0.701201]\n",
      "[Epoch 46/100] [Batch 0/2] [D loss: 0.693064] [G loss: 0.703922]\n",
      "[Epoch 46/100] [Batch 1/2] [D loss: 0.680302] [G loss: 0.699218]\n",
      "[Epoch 47/100] [Batch 0/2] [D loss: 0.680900] [G loss: 0.717718]\n",
      "[Epoch 47/100] [Batch 1/2] [D loss: 0.684993] [G loss: 0.707812]\n",
      "[Epoch 48/100] [Batch 0/2] [D loss: 0.686571] [G loss: 0.716689]\n",
      "[Epoch 48/100] [Batch 1/2] [D loss: 0.682987] [G loss: 0.715269]\n",
      "[Epoch 49/100] [Batch 0/2] [D loss: 0.678504] [G loss: 0.725908]\n",
      "[Epoch 49/100] [Batch 1/2] [D loss: 0.692672] [G loss: 0.700051]\n",
      "[Epoch 50/100] [Batch 0/2] [D loss: 0.682673] [G loss: 0.717629]\n",
      "[Epoch 50/100] [Batch 1/2] [D loss: 0.686657] [G loss: 0.711993]\n",
      "[Epoch 51/100] [Batch 0/2] [D loss: 0.680677] [G loss: 0.714215]\n",
      "[Epoch 51/100] [Batch 1/2] [D loss: 0.684309] [G loss: 0.712813]\n",
      "[Epoch 52/100] [Batch 0/2] [D loss: 0.676300] [G loss: 0.726056]\n",
      "[Epoch 52/100] [Batch 1/2] [D loss: 0.679536] [G loss: 0.722136]\n",
      "[Epoch 53/100] [Batch 0/2] [D loss: 0.685455] [G loss: 0.720767]\n",
      "[Epoch 53/100] [Batch 1/2] [D loss: 0.676735] [G loss: 0.729816]\n",
      "[Epoch 54/100] [Batch 0/2] [D loss: 0.680260] [G loss: 0.721917]\n",
      "[Epoch 54/100] [Batch 1/2] [D loss: 0.680883] [G loss: 0.718537]\n",
      "[Epoch 55/100] [Batch 0/2] [D loss: 0.686114] [G loss: 0.717019]\n",
      "[Epoch 55/100] [Batch 1/2] [D loss: 0.681684] [G loss: 0.724016]\n",
      "[Epoch 56/100] [Batch 0/2] [D loss: 0.685618] [G loss: 0.723102]\n",
      "[Epoch 56/100] [Batch 1/2] [D loss: 0.678405] [G loss: 0.723594]\n",
      "[Epoch 57/100] [Batch 0/2] [D loss: 0.676718] [G loss: 0.717650]\n",
      "[Epoch 57/100] [Batch 1/2] [D loss: 0.685736] [G loss: 0.712553]\n",
      "[Epoch 58/100] [Batch 0/2] [D loss: 0.685233] [G loss: 0.698506]\n",
      "[Epoch 58/100] [Batch 1/2] [D loss: 0.689464] [G loss: 0.696352]\n",
      "[Epoch 59/100] [Batch 0/2] [D loss: 0.684666] [G loss: 0.703911]\n",
      "[Epoch 59/100] [Batch 1/2] [D loss: 0.680737] [G loss: 0.697588]\n",
      "[Epoch 60/100] [Batch 0/2] [D loss: 0.669488] [G loss: 0.704237]\n",
      "[Epoch 60/100] [Batch 1/2] [D loss: 0.691656] [G loss: 0.688410]\n",
      "[Epoch 61/100] [Batch 0/2] [D loss: 0.681489] [G loss: 0.688257]\n",
      "[Epoch 61/100] [Batch 1/2] [D loss: 0.686333] [G loss: 0.682383]\n",
      "[Epoch 62/100] [Batch 0/2] [D loss: 0.683333] [G loss: 0.686597]\n",
      "[Epoch 62/100] [Batch 1/2] [D loss: 0.686525] [G loss: 0.681293]\n",
      "[Epoch 63/100] [Batch 0/2] [D loss: 0.684154] [G loss: 0.683321]\n",
      "[Epoch 63/100] [Batch 1/2] [D loss: 0.682903] [G loss: 0.690524]\n",
      "[Epoch 64/100] [Batch 0/2] [D loss: 0.679769] [G loss: 0.700212]\n",
      "[Epoch 64/100] [Batch 1/2] [D loss: 0.690862] [G loss: 0.684088]\n",
      "[Epoch 65/100] [Batch 0/2] [D loss: 0.689660] [G loss: 0.690072]\n",
      "[Epoch 65/100] [Batch 1/2] [D loss: 0.682505] [G loss: 0.706498]\n",
      "[Epoch 66/100] [Batch 0/2] [D loss: 0.689320] [G loss: 0.698831]\n",
      "[Epoch 66/100] [Batch 1/2] [D loss: 0.677125] [G loss: 0.707009]\n",
      "[Epoch 67/100] [Batch 0/2] [D loss: 0.685685] [G loss: 0.702092]\n",
      "[Epoch 67/100] [Batch 1/2] [D loss: 0.677584] [G loss: 0.724766]\n",
      "[Epoch 68/100] [Batch 0/2] [D loss: 0.681164] [G loss: 0.719735]\n",
      "[Epoch 68/100] [Batch 1/2] [D loss: 0.683750] [G loss: 0.707229]\n",
      "[Epoch 69/100] [Batch 0/2] [D loss: 0.683230] [G loss: 0.720325]\n",
      "[Epoch 69/100] [Batch 1/2] [D loss: 0.686279] [G loss: 0.724295]\n",
      "[Epoch 70/100] [Batch 0/2] [D loss: 0.686942] [G loss: 0.728260]\n",
      "[Epoch 70/100] [Batch 1/2] [D loss: 0.696653] [G loss: 0.718339]\n",
      "[Epoch 71/100] [Batch 0/2] [D loss: 0.702654] [G loss: 0.723517]\n",
      "[Epoch 71/100] [Batch 1/2] [D loss: 0.704125] [G loss: 0.727098]\n",
      "[Epoch 72/100] [Batch 0/2] [D loss: 0.696696] [G loss: 0.728909]\n",
      "[Epoch 72/100] [Batch 1/2] [D loss: 0.700951] [G loss: 0.733484]\n",
      "[Epoch 73/100] [Batch 0/2] [D loss: 0.698251] [G loss: 0.728123]\n",
      "[Epoch 73/100] [Batch 1/2] [D loss: 0.695276] [G loss: 0.730571]\n",
      "[Epoch 74/100] [Batch 0/2] [D loss: 0.699647] [G loss: 0.717829]\n",
      "[Epoch 74/100] [Batch 1/2] [D loss: 0.692042] [G loss: 0.721424]\n",
      "[Epoch 75/100] [Batch 0/2] [D loss: 0.698144] [G loss: 0.715399]\n",
      "[Epoch 75/100] [Batch 1/2] [D loss: 0.695639] [G loss: 0.704258]\n",
      "[Epoch 76/100] [Batch 0/2] [D loss: 0.697531] [G loss: 0.705451]\n",
      "[Epoch 76/100] [Batch 1/2] [D loss: 0.694885] [G loss: 0.704955]\n",
      "[Epoch 77/100] [Batch 0/2] [D loss: 0.693336] [G loss: 0.710531]\n",
      "[Epoch 77/100] [Batch 1/2] [D loss: 0.689293] [G loss: 0.704133]\n",
      "[Epoch 78/100] [Batch 0/2] [D loss: 0.691020] [G loss: 0.704149]\n",
      "[Epoch 78/100] [Batch 1/2] [D loss: 0.692357] [G loss: 0.706034]\n",
      "[Epoch 79/100] [Batch 0/2] [D loss: 0.696683] [G loss: 0.698206]\n",
      "[Epoch 79/100] [Batch 1/2] [D loss: 0.701589] [G loss: 0.685418]\n",
      "[Epoch 80/100] [Batch 0/2] [D loss: 0.703105] [G loss: 0.690610]\n",
      "[Epoch 80/100] [Batch 1/2] [D loss: 0.692645] [G loss: 0.704633]\n",
      "[Epoch 81/100] [Batch 0/2] [D loss: 0.699211] [G loss: 0.697092]\n",
      "[Epoch 81/100] [Batch 1/2] [D loss: 0.692984] [G loss: 0.698676]\n",
      "[Epoch 82/100] [Batch 0/2] [D loss: 0.698687] [G loss: 0.692372]\n",
      "[Epoch 82/100] [Batch 1/2] [D loss: 0.692586] [G loss: 0.694034]\n",
      "[Epoch 83/100] [Batch 0/2] [D loss: 0.690035] [G loss: 0.699112]\n",
      "[Epoch 83/100] [Batch 1/2] [D loss: 0.695714] [G loss: 0.686980]\n",
      "[Epoch 84/100] [Batch 0/2] [D loss: 0.687405] [G loss: 0.698646]\n",
      "[Epoch 84/100] [Batch 1/2] [D loss: 0.695709] [G loss: 0.690800]\n",
      "[Epoch 85/100] [Batch 0/2] [D loss: 0.692550] [G loss: 0.694181]\n",
      "[Epoch 85/100] [Batch 1/2] [D loss: 0.687788] [G loss: 0.699769]\n",
      "[Epoch 86/100] [Batch 0/2] [D loss: 0.689630] [G loss: 0.693567]\n",
      "[Epoch 86/100] [Batch 1/2] [D loss: 0.687313] [G loss: 0.698345]\n",
      "[Epoch 87/100] [Batch 0/2] [D loss: 0.690155] [G loss: 0.691782]\n",
      "[Epoch 87/100] [Batch 1/2] [D loss: 0.683243] [G loss: 0.704798]\n",
      "[Epoch 88/100] [Batch 0/2] [D loss: 0.678850] [G loss: 0.710791]\n",
      "[Epoch 88/100] [Batch 1/2] [D loss: 0.683034] [G loss: 0.707581]\n",
      "[Epoch 89/100] [Batch 0/2] [D loss: 0.686794] [G loss: 0.699263]\n",
      "[Epoch 89/100] [Batch 1/2] [D loss: 0.677615] [G loss: 0.715213]\n",
      "[Epoch 90/100] [Batch 0/2] [D loss: 0.685993] [G loss: 0.702171]\n",
      "[Epoch 90/100] [Batch 1/2] [D loss: 0.680693] [G loss: 0.710847]\n",
      "[Epoch 91/100] [Batch 0/2] [D loss: 0.681947] [G loss: 0.720458]\n",
      "[Epoch 91/100] [Batch 1/2] [D loss: 0.679792] [G loss: 0.726026]\n",
      "[Epoch 92/100] [Batch 0/2] [D loss: 0.686960] [G loss: 0.719129]\n",
      "[Epoch 92/100] [Batch 1/2] [D loss: 0.674244] [G loss: 0.723632]\n",
      "[Epoch 93/100] [Batch 0/2] [D loss: 0.681520] [G loss: 0.724914]\n",
      "[Epoch 93/100] [Batch 1/2] [D loss: 0.688871] [G loss: 0.709740]\n",
      "[Epoch 94/100] [Batch 0/2] [D loss: 0.681024] [G loss: 0.728353]\n",
      "[Epoch 94/100] [Batch 1/2] [D loss: 0.677333] [G loss: 0.736367]\n",
      "[Epoch 95/100] [Batch 0/2] [D loss: 0.684324] [G loss: 0.724076]\n",
      "[Epoch 95/100] [Batch 1/2] [D loss: 0.678220] [G loss: 0.727713]\n",
      "[Epoch 96/100] [Batch 0/2] [D loss: 0.684796] [G loss: 0.710458]\n",
      "[Epoch 96/100] [Batch 1/2] [D loss: 0.685407] [G loss: 0.714927]\n",
      "[Epoch 97/100] [Batch 0/2] [D loss: 0.684619] [G loss: 0.717447]\n",
      "[Epoch 97/100] [Batch 1/2] [D loss: 0.685503] [G loss: 0.714859]\n",
      "[Epoch 98/100] [Batch 0/2] [D loss: 0.688580] [G loss: 0.713746]\n",
      "[Epoch 98/100] [Batch 1/2] [D loss: 0.678498] [G loss: 0.736055]\n",
      "[Epoch 99/100] [Batch 0/2] [D loss: 0.681450] [G loss: 0.727871]\n",
      "[Epoch 99/100] [Batch 1/2] [D loss: 0.684018] [G loss: 0.739222]\n"
     ]
    }
   ],
   "source": [
    "rel_entr_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    rel_entr = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_data = Variable(data.type(Tensor))\n",
    "        # Generate a batch of images\n",
    "        gen_data = generator()\n",
    "\n",
    "        # Evaluate Relative Entropy\n",
    "        rel_entr.append(get_rel_entr(gen_data))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        disc_data = discriminator(real_data)\n",
    "        real_loss = d_loss_fun(disc_data, valid)\n",
    "        fake_loss = d_loss_fun(discriminator(gen_data), fake)  # (discriminator(gen_data).detach(), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # for name, param in discriminator.state_dict().items():\n",
    "        #     print(name, param.detach())\n",
    "\n",
    "        # print('d loss ', d_loss)\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = g_loss_fun(discriminator(gen_data), valid)\n",
    "        g_loss.retain_grad = True\n",
    "        g_loss_grad = g_loss_fun_grad(qnn, ansatz.ordered_parameters, generator.weight.data.numpy(), discriminator)\n",
    "\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        for j, param in enumerate(generator.parameters()):\n",
    "            param.grad = g_loss_grad\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        # batches_done = epoch * len(dataloader) + i\n",
    "        # if batches_done % optimizer_G.sample_interval == 0:\n",
    "        #     #TODO: Do something like storing, printing or relative entropy evaluation\n",
    "        #     pass\n",
    "\n",
    "    rel_entr_list.append(np.mean(rel_entr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative entropy  [1.712349361153411, 1.722884709742737, 1.8838426106715145, 1.7601374477520324, 1.7627355670205012, 1.6091215156891399, 1.7466653725276133, 1.5988352159301735, 1.6353539159728574, 1.3420077218204036, 1.8984818120714726, 1.3864323378254242, 1.2883422149745736, 1.3675463970821973, 1.5198449178797258, 1.245582986736179, 1.264990207520007, 1.2122216957553764, 1.2627542370372962, 1.1659676771217167, 1.2983495878869935, 1.0956322038294535, 1.1213118812840048, 1.0537963658350087, 1.1128482298071751, 1.0716683157514575, 1.0379317870397275, 1.1368614220038378, 1.1306008315862761, 1.1902810696074613, 1.0774867320821304, 1.0427759778439984, 1.1025904307460834, 1.0397652189508277, 0.9782199413039465, 0.948812078230151, 0.9405030161344059, 0.9507497549949461, 0.9246165329049216, 0.9313139484692454, 0.9039420936925577, 0.9832687933294664, 0.9681168778001941, 0.9226553255154323, 0.8963672096421159, 0.9684335415086522, 0.9682423820414322, 0.9681959147930519, 0.9886157462190738, 0.9700766051055931, 0.9495915594259428, 0.98211556813347, 0.9406929220954183, 0.9395890210316864, 0.9497050244280726, 1.0107324097713368, 0.9193040118945157, 0.8907952111052261, 0.923307509012637, 0.9614515349139758, 0.9292478297977327, 0.9487700710984732, 0.996212242623975, 0.9734428893830096, 0.9164102510819836, 0.9144971970749217, 0.9031201343196975, 0.8940323803520365, 0.9154143370940607, 0.9168404516521236, 0.9184471457399946, 0.9468746712467989, 0.8705185739562354, 0.9176653690869416, 0.845469773303084, 0.8888979229248517, 0.9362198541950884, 0.9143755913792406, 0.9081429777798278, 0.9368178477280296, 1.0018126474616524, 1.0019074109576032, 0.9479797337649835, 0.8458499833773856, 1.0224401967479295, 1.0260901143632615, 0.9502140532268748, 0.9766022295445413, 1.0729412887037788, 0.9275213591716694, 0.8702861232992554, 0.8966216059905379, 1.0684793224733469, 0.8752126121997643, 0.9873420434101056, 0.9835960371043713, 0.9002059637223947, 0.7911216849659175, 0.8338754540521505, 0.8643794911086818]\n"
     ]
    }
   ],
   "source": [
    "print('Relative entropy ', rel_entr_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "[[-1.6184392680888893, -1.5402153632625417],\n [-1.6184392680888893, -1.0891895102937574],\n [-1.6184392680888893, -0.638163657324973],\n [-1.6184392680888893, -0.18713780435618865],\n [-1.6184392680888893, 0.2638880486125956],\n [-1.6184392680888893, 0.7149139015813799],\n [-1.6184392680888893, 1.1659397545501644],\n [-1.6184392680888893, 1.6169656075189487],\n [-1.1450048106382602, -1.5402153632625417],\n [-1.1450048106382602, -1.0891895102937574],\n [-1.1450048106382602, -0.638163657324973],\n [-1.1450048106382602, -0.18713780435618865],\n [-1.1450048106382602, 0.2638880486125956],\n [-1.1450048106382602, 0.7149139015813799],\n [-1.1450048106382602, 1.1659397545501644],\n [-1.1450048106382602, 1.6169656075189487],\n [-0.6715703531876313, -1.5402153632625417],\n [-0.6715703531876313, -1.0891895102937574],\n [-0.6715703531876313, -0.638163657324973],\n [-0.6715703531876313, -0.18713780435618865],\n [-0.6715703531876313, 0.2638880486125956],\n [-0.6715703531876313, 0.7149139015813799],\n [-0.6715703531876313, 1.1659397545501644],\n [-0.6715703531876313, 1.6169656075189487],\n [-0.1981358957370023, -1.5402153632625417],\n [-0.1981358957370023, -1.0891895102937574],\n [-0.1981358957370023, -0.638163657324973],\n [-0.1981358957370023, -0.18713780435618865],\n [-0.1981358957370023, 0.2638880486125956],\n [-0.1981358957370023, 0.7149139015813799],\n [-0.1981358957370023, 1.1659397545501644],\n [-0.1981358957370023, 1.6169656075189487],\n [0.27529856171362677, -1.5402153632625417],\n [0.27529856171362677, -1.0891895102937574],\n [0.27529856171362677, -0.638163657324973],\n [0.27529856171362677, -0.18713780435618865],\n [0.27529856171362677, 0.2638880486125956],\n [0.27529856171362677, 0.7149139015813799],\n [0.27529856171362677, 1.1659397545501644],\n [0.27529856171362677, 1.6169656075189487],\n [0.7487330191642556, -1.5402153632625417],\n [0.7487330191642556, -1.0891895102937574],\n [0.7487330191642556, -0.638163657324973],\n [0.7487330191642556, -0.18713780435618865],\n [0.7487330191642556, 0.2638880486125956],\n [0.7487330191642556, 0.7149139015813799],\n [0.7487330191642556, 1.1659397545501644],\n [0.7487330191642556, 1.6169656075189487],\n [1.2221674766148847, -1.5402153632625417],\n [1.2221674766148847, -1.0891895102937574],\n [1.2221674766148847, -0.638163657324973],\n [1.2221674766148847, -0.18713780435618865],\n [1.2221674766148847, 0.2638880486125956],\n [1.2221674766148847, 0.7149139015813799],\n [1.2221674766148847, 1.1659397545501644],\n [1.2221674766148847, 1.6169656075189487],\n [1.6956019340655135, -1.5402153632625417],\n [1.6956019340655135, -1.0891895102937574],\n [1.6956019340655135, -0.638163657324973],\n [1.6956019340655135, -0.18713780435618865],\n [1.6956019340655135, 0.2638880486125956],\n [1.6956019340655135, 0.7149139015813799],\n [1.6956019340655135, 1.1659397545501644],\n [1.6956019340655135, 1.6169656075189487]]"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(prob_data)\n",
    "# data_grid\n",
    "grid_elements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternative approach\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QiskitDevenv",
   "language": "python",
   "name": "qiskitdevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}